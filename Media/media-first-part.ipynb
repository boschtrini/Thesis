{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# First analysis part\n",
    "    Preprocess text\n",
    "    Word Frequency\n",
    "    Bigrams\n",
    "    W2V embeddings for NN\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard data manipulation and text processing libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.probability import FreqDist\n",
    "from nltk.corpus import words\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# Download NLTK datasets\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('words')\n",
    "\n",
    "# Stop words and language processing tools\n",
    "stop_words = set(stopwords.words('english'))\n",
    "stop_words.add(\"None\")\n",
    "\n",
    "# Spacy for advanced text processing\n",
    "import spacy\n",
    "\n",
    "# Load Spacy language model\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# Gensim for word embeddings\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "# Dimensionality reduction\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Visualization libraries\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Multiprocessing for parallelization\n",
    "from multiprocessing import cpu_count\n",
    "import concurrent.futures\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Other utilities\n",
    "from collections import Counter\n",
    "import json\n",
    "import string"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##load data\n",
    "data = pd.read_pickle('/Users/trinidadbosch/Desktop/SEDS/MA-Thesis/thesis-env/Media Cloud/Data/Final Data/final-news-data.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocess text\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load spaCy model with only the tokenizer, tagger, and lemmatizer enabled\n",
    "nlp = spacy.load('en_core_web_sm', disable=['parser', 'ner'])\n",
    "\n",
    "# Function to clean and lemmatize text\n",
    "def preprocessing(texts):  # takes a list of texts as input\n",
    "    cleaned_texts = (re.sub(r'[^a-zA-Z0-9\\s]|(\\n+)', ' ', str(text)).strip() for text in texts)\n",
    "    \n",
    "    # Process texts as a stream using nlp.pipe\n",
    "    for doc in nlp.pipe(cleaned_texts, batch_size=100):  # batch size can be adjusted\n",
    "        yield [token.lemma_.lower() for token in doc]\n",
    "#Function to remove stopwords from a list of tokens\n",
    "def remove_stopwords(tokens):\n",
    "    return [token for token in tokens if token.lower() not in stop_words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply preprocessing function to 'Body' and 'Title' columns in batches\n",
    "data['Cleaned_Body'] = list(preprocessing(data['Body'].tolist()))\n",
    "data['Cleaned_Title'] = list(preprocessing(data['Title'].tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply remove_stopwords \n",
    "data['Cleaned_Body_sw'] = data['Cleaned_Body'].apply(remove_stopwords)\n",
    "data['Cleaned_Title_sw'] = data['Cleaned_Title'].apply(remove_stopwords)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocess dates\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.collect_date.isnull().any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspect unique date formats to find inconsistencies\n",
    "# This helps in understanding the variety of formats present\n",
    "unique_formats = data['collect_date'].apply(lambda x: x.strip()).unique()\n",
    "print(unique_formats[:10])  # Print a sample of unique date formats to check for inconsistencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['collect_date_'] = pd.to_datetime(data['collect_date'], errors='coerce')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nan_rows = data[data['collect_date_'].isna()]\n",
    "print(nan_rows['collect_date_'])  # Assuming the original 'collect_date' is still a string here or use another identifier column.\n",
    "nan_rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nan_rows['collect_date_']= pd.to_datetime(nan_rows['collect_date'], format='%Y-%m-%d %H:%M:%S',errors='coerce')\n",
    "nan_rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## drop column\n",
    "data = data.dropna(subset=['collect_date_'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenate the cleaned_data DataFrame with the nan_rows DataFrame\n",
    "data = pd.concat([data, nan_rows], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Now extract the year\n",
    "data['collect_year'] = data['collect_date_'].dt.year\n",
    "data['collect_month'] = data['collect_date_'].dt.month\n",
    "\n",
    "min_year = data['collect_year'].min()\n",
    "max_year = data['collect_year'].max()\n",
    "\n",
    "print(f\"Minimum year: {min_year}\")\n",
    "print(f\"Maximum year: {max_year}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.drop(columns=['collect_date', 'feeds'])\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function to remove stopwords from a list of tokens\n",
    "def remove_stopwords(tokens):\n",
    "    return [token for token in tokens if token.lower() not in stop_words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply remove_stopwords \n",
    "data['Cleaned_Body_sw'] = data['Cleaned_Body'].apply(remove_stopwords)\n",
    "data['Cleaned_Title_sw'] = data['Cleaned_Title'].apply(remove_stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.to_pickle('media-data.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Analysis\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Part 1: Word Frequency\n",
    "    Word Frequency Statistics: Media Articles\n",
    "\n",
    "    In this section I'll do a general descriptive analysis of my dataset of AI Media Articles. This description will give more insights about the words and check wether I need to include other terms to the original query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plotting count of news per year\n",
    "counts_per_year = data['collect_year'].value_counts().sort_index()\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "ax = counts_per_year.plot(kind='bar')\n",
    "plt.xlabel('Year')\n",
    "plt.ylabel('Count')\n",
    "plt.title('Media Articles Count per Year')\n",
    "\n",
    "# Annotate each bar with its count\n",
    "for i, count in enumerate(counts_per_year):\n",
    "    ax.text(i, count + 1, str(count), ha='center', va='bottom')\n",
    "\n",
    "plt.savefig('media-count-year.svg', format='svg')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_space_strings(word_list):\n",
    "    \"\"\"\n",
    "    Removes strings that consist solely of spaces from a list of words.\n",
    "    \"\"\"\n",
    "    filter_words = [' ', '  ', '   ', '    ','        ']  # Define the strings you want to filter out\n",
    "    return [word for word in word_list if word not in filter_words]\n",
    "\n",
    "# Apply the filtering function to each row of the 'Cleaned_Body_sw' and 'Cleaned_Title_sw' columns\n",
    "data['Cleaned_Body_sw'] = data['Cleaned_Body_sw'].apply(filter_space_strings)\n",
    "data['Cleaned_Title_sw'] = data['Cleaned_Title_sw'].apply(filter_space_strings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## counting words\n",
    "from collections import Counter\n",
    "\n",
    "body_words = [word for words in data['Cleaned_Body_sw'] for word in words] \n",
    "title_words = [word for words in data['Cleaned_Title_sw'] for word in words]\n",
    "\n",
    "body_word_freq = Counter(body_words)\n",
    "title_word_freq = Counter(title_words)\n",
    "\n",
    "body_word_freq_dict = dict(body_word_freq)\n",
    "title_word_freq_dict = dict(title_word_freq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Assuming body_word_freq and title_word_freq are Counter objects or similar\n",
    "# If they are dictionaries, convert them to Counter objects for simplicity\n",
    "body_word_freq = Counter(body_word_freq)\n",
    "title_word_freq = Counter(title_word_freq)\n",
    "\n",
    "#filter_words = ['   ','  ',' ','']\n",
    "\n",
    "# Filter out specific words and get the most common 20\n",
    "filtered_body_common_words = Counter({word: count for word, count in body_word_freq.items()})\n",
    "filtered_title_common_words = Counter({word: count for word, count in title_word_freq.items()})\n",
    "\n",
    "# Plot the most common words in titles\n",
    "title_common_words = filtered_title_common_words.most_common(20)\n",
    "title_word, title_count = zip(*title_common_words)\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.bar(title_word, title_count)\n",
    "plt.title(\"Top 20 Words in Titles\")\n",
    "plt.xlabel(\"Words\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.xticks(rotation=90,fontsize=14)\n",
    "\n",
    "# # Annotate each bar with its count (smaller font size)\n",
    "# for i, count in enumerate(title_count):\n",
    "#     plt.text(i, count + 0.1, str(count), ha='center', va='bottom', fontsize=8)\n",
    "\n",
    "plt.savefig('top-20-words-title.svg', format='svg')\n",
    "plt.show()\n",
    "\n",
    "# Plot the most common words in bodies\n",
    "abstract_common_words = filtered_body_common_words.most_common(20)\n",
    "abstract_word, abstract_count = zip(*abstract_common_words)\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.bar(abstract_word, abstract_count)\n",
    "plt.title(\"Top 20 Words in Body\")\n",
    "plt.xlabel(\"Words\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.xticks(rotation=90,fontsize=14)\n",
    "\n",
    "# # Annotate each bar with its count\n",
    "# for i, count in enumerate(abstract_count):\n",
    "#     plt.text(i, count + 0.1, str(count), ha='center', va='bottom', fontsize=8)\n",
    "\n",
    "plt.savefig('top-20-words-body.svg', format='svg')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from nltk.probability import FreqDist\n",
    "\n",
    "# Assuming title_words and body_words are defined and combined into all_tokens\n",
    "all_tokens = body_word_freq + title_word_freq\n",
    "freq_dist = FreqDist(all_tokens)\n",
    "\n",
    "# Get the most common words and their frequencies for plotting\n",
    "most_common_words = freq_dist.most_common(30)  # Adjust N as needed\n",
    "words, frequencies = zip(*most_common_words)  # Unpacks list of tuples\n",
    "\n",
    "# Prepare the data for a bar chart\n",
    "plt.figure(figsize=(12, 6))\n",
    "bars = plt.bar(range(len(words)), frequencies, color='lightblue')  # Create a bar chart\n",
    "\n",
    "# Adjust x-axis limits to remove space before the first bar and after the last one\n",
    "plt.xlim(-0.5, len(words) - 0.5)\n",
    "\n",
    "# Set x-ticks, rotate labels for readability, and set font size\n",
    "plt.xticks(range(len(words)), words, rotation=90, fontsize=14)\n",
    "plt.yticks(fontsize=12)\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "plt.savefig('top-30-words-media.svg', format='svg')\n",
    "# Show the plot\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Bigrams\n",
    "    Exploring which words act like collocations grammarly in my corpus\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#make sure to lowercase everything\n",
    "title_lemma = data.Cleaned_Title_sw\n",
    "body_lemma = data.Cleaned_Body_sw\n",
    "\n",
    "all_text = title_lemma+body_lemma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#turn all text' tokens into one single list\n",
    "unlist_comments = [item for items in all_text for item in items]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unlist_comments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize NLTK's Bigrams/Trigrams Finder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bigrams = nltk.collocations.BigramAssocMeasures()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bigramFinder = nltk.collocations.BigramCollocationFinder.from_words(unlist_comments)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Counting Frequencies of Adjacent Words\n",
    "- Main idea: simply order by frequency\n",
    "- Issues: too sensitive to very frequent pairs and pronouns/articles/prepositions come up often\n",
    "- Solution: filter for only adjectives and nouns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bigram_freq = bigramFinder.ngram_fd.items()\n",
    "bigramFreqTable = pd.DataFrame(list(bigram_freq), columns=['bigram','freq']).sort_values(by='freq', ascending=False)\n",
    "bigramFreqTable.head(50).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#function to filter for ADJ/NN bigrams\n",
    "def rightTypes(ngram):\n",
    "    if '-pron-' in ngram or '' in ngram or ' 'in ngram or 't' in ngram:\n",
    "        return False\n",
    "    for word in ngram:\n",
    "        if word in stop_words:\n",
    "            return False\n",
    "    acceptable_types = ('JJ', 'JJR', 'JJS', 'NN', 'NNS', 'NNP', 'NNPS')\n",
    "    second_type = ('NN', 'NNS', 'NNP', 'NNPS')\n",
    "    tags = nltk.pos_tag(ngram)\n",
    "    if tags[0][1] in acceptable_types and tags[1][1] in second_type:\n",
    "        return True\n",
    "    else:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#filter bigrams\n",
    "filtered_bi = bigramFreqTable[bigramFreqTable.bigram.map(lambda x: rightTypes(x))]\n",
    "filtered_bi.head().reset_index(drop=True)\n",
    "filtered_bi.to_csv('bigram_news.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_bi.head(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter top 100 bigrams\n",
    "top_100_bigrams = bigrams.sort_values(by='freq', ascending=False).head(100)\n",
    "top_100_bigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple function to convert bigram strings\n",
    "def simple_convert_bigram(bigram_str):\n",
    "    # Split the string by comma, remove unwanted characters, and join with an underscore\n",
    "    return '_'.join(bigram_str.replace(\"('\", \"\").replace(\"')\", \"\").replace(\"'\", \"\").split(\", \"))\n",
    "\n",
    "# Apply the conversion function to each bigram\n",
    "top_100_bigrams['bigram'] = top_100_bigrams['bigram'].apply(simple_convert_bigram)\n",
    "\n",
    "# Now `df['bigram']` contains the bigrams in the desired format\n",
    "print(top_100_bigrams.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the 'bigram' column to a list\n",
    "bigrams_list = bigrams['bigram'].tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## NER to categorize the bigrams\n",
    "    This analysis I didn't use it in the final document, but I'm leaving it here because why not\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def categorize_bigram(bigram):\n",
    "    # Categories and their associated bigrams\n",
    "    categories = {\n",
    "        \"Artificial Intelligence & Technology\": [\n",
    "            'artificial_intelligence', 'generative_ai', 'use_ai', 'machine_learning', 'machine_learn', 'ai_system',\n",
    "            'tech_company', 'intelligence_ai', 'facial_recognition', 'deep_learning', 'ai_tool', 'ai_technology',\n",
    "            'language_model', 'ai_model', 'ai_generate', 'essential_ai', 'arm_chips', 'ai_phones', 'open_source',\n",
    "            'natural_language', 'digital_transformation', 'large_language', 'tech_giant', 'use_technology',\n",
    "            'computer_science', 'self_driving', 'business_model', 'next_generation', 'ai_use', 'technology_company',\n",
    "            'privacy_policy', 'language_processing', 'image_credit','drive_car', 'answer_question', 'amp_amp', 'video_game'\n",
    "        ],\n",
    "        \"Geographical Locations\": [\n",
    "            'new_york', 'united_states', 'white_house', 'silicon_valley', 'wall_street', 'san_francisco',\n",
    "            'los_angeles', 'u_k'\n",
    "        ],\n",
    "        \"People\": [\n",
    "            'elon_musk', 'vice_president', 'co_founder', 'donald_trump', 'joe_biden', 'chief_executive',\n",
    "            'biden_administration','phones_ceo','use_case'\n",
    "        ],\n",
    "        \"Business & Economy\": [\n",
    "            'fiscal_year', 'supply_chain', 'national_security', 'climate_change', 'customer_service',\n",
    "            'law_enforcement', 'e_commerce', 'datum_center', 'customer_experience', 'medium_platform',\n",
    "            'interest_rate', 'real_estate', 'business_model', 'decision_make'\n",
    "        ],\n",
    "        \"Society & Culture\": [\n",
    "            'social_medium', 'health_care', 'climate_change', 'sag_aftra', 'common_myth', 'world_creativity',\n",
    "            'myth_genius', 'divine_inspiration', 'genius_strike', 'real_world', 'mental_health',\n",
    "            'washington_post', 'york_times','fox_news', 'many_people'\n",
    "        ],\n",
    "        \"Time\": [\n",
    "            'last_year', 'last_week', 'long_term', 'first_time','last_month', 'next_year',\n",
    "            'third_party', 'early_year',\n",
    "            'recent_year', 'make_sense'\n",
    "        ]\n",
    "    }\n",
    "\n",
    "    # Determine the category for the given bigram\n",
    "    for category, bigrams in categories.items():\n",
    "        if bigram in bigrams:\n",
    "            return category\n",
    "    return \"Uncategorized\"  # Return this if the bigram doesn't fit in any category\n",
    "\n",
    "# Example usage\n",
    "# Assuming df is your DataFrame and 'bigram' is the column with bigrams\n",
    "top_100_bigrams['category'] = top_100_bigrams['bigram'].apply(categorize_bigram)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_100_bigrams.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from wordcloud import WordCloud\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Assuming 'df' is your DataFrame after categorization and it includes a 'Freq' column for frequency\n",
    "# Let's add a logarithmic frequency column to the DataFrame for better visualization\n",
    "top_100_bigrams['Log_Freq'] = np.log(top_100_bigrams['freq'])\n",
    "\n",
    "# Define a color map based on categories\n",
    "category_colors = {\n",
    "    'Artificial Intelligence & Technology': 'dodgerblue',\n",
    "    'Geographical Locations': 'limegreen',\n",
    "    'People': 'gold',\n",
    "    'Business & Economy': 'red',\n",
    "    'Society & Culture': 'purple',\n",
    "    'Time': 'grey',\n",
    "    'Uncategorized': 'black'  # In case there are any uncategorized bigrams\n",
    "}\n",
    "\n",
    "# Function to apply colors based on categories\n",
    "def color_func(word, **kwargs):\n",
    "    category = top_100_bigrams[top_100_bigrams['bigram'] == word]['category'].values[0]\n",
    "    return category_colors.get(category, 'black')\n",
    "\n",
    "# Generate the word cloud\n",
    "wordcloud = WordCloud(width=800, height=400, background_color='white', color_func=color_func)\n",
    "\n",
    "# Prepare the frequencies in the correct format for the word cloud\n",
    "frequencies = {row['bigram']: row['Log_Freq'] for index, row in top_100_bigrams.iterrows()}\n",
    "wordcloud.generate_from_frequencies(frequencies)\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(15, 10))\n",
    "plt.imshow(wordcloud, interpolation='bilinear')\n",
    "plt.axis('off')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming you have your 'top_100_bigrams' DataFrame already set up\n",
    "\n",
    "category_colors = {\n",
    "    'Artificial Intelligence & Technology': '#000066',\n",
    "    'Geographical Locations': '#5d1f0c',\n",
    "    'People': '#cc0099',\n",
    "    'Business & Economy': '#cc9900',\n",
    "    'Society & Culture': '#006600',\n",
    "    'Time': '#66ccff',\n",
    "    'Uncategorized': 'lightgrey'\n",
    "}\n",
    "\n",
    "# Ensure the DataFrame has a 'log_freq' column\n",
    "top_100_bigrams['log_freq'] = np.log(top_100_bigrams['freq'])\n",
    "\n",
    "# Sort the DataFrame by the 'log_freq' in descending order and take the top 50\n",
    "top_bigrams = top_100_bigrams.sort_values(by='log_freq', ascending=False).head(50)\n",
    "\n",
    "# Create a mapping of the top bigrams to their colors using the DataFrame\n",
    "bigram_to_color = {bigram: category_colors[category] for bigram, category in zip(top_bigrams['bigram'], top_bigrams['category'])}\n",
    "\n",
    "# Define a custom color function to use with the word cloud\n",
    "def custom_color_func(word, *args, **kwargs):\n",
    "    return bigram_to_color.get(word, 'lightgrey')  # Default to 'lightgrey' if the word is not found\n",
    "\n",
    "# Create a word cloud with mixed orientations\n",
    "wordcloud = WordCloud(width=800, height=400, prefer_horizontal=0.2, color_func=custom_color_func, background_color='white')\n",
    "\n",
    "# Generate the word cloud using the top bigrams' frequencies\n",
    "wordcloud.generate_from_frequencies(dict(zip(top_bigrams['bigram'], top_bigrams['log_freq'])))\n",
    "\n",
    "# Save the word cloud as PNG\n",
    "wordcloud.to_file('wordcloud-media.png')\n",
    "\n",
    "# Display the word cloud\n",
    "plt.figure(figsize=(15, 10))\n",
    "plt.imshow(wordcloud, interpolation='bilinear')\n",
    "plt.axis('off')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## W2V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine the two columns into one\n",
    "data['combined_text'] = data['Cleaned_Body_sw'] + data['Cleaned_Title_sw']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "## Keeping certain n-grams together for later NN analysis\n",
    "\n",
    "# Define  n-grams\n",
    "n_grams_to_keep = [\n",
    "    \"neural network\",\n",
    "    \"deep learning\",\n",
    "    \"reinforcement learning\",\n",
    "    \"supervised learning\",\n",
    "    \"unsupervised learning\",\n",
    "    \"generative model\",\n",
    "    \"natural language processing\",\n",
    "    \"artificial intelligence\"\n",
    "]\n",
    "\n",
    "# Function to replace spaces with underscores in n-grams\n",
    "def preserve_n_grams(tokens, n_grams_to_keep):\n",
    "    n_grams_dict = {n_gram: n_gram.replace(' ', '_') for n_gram in n_grams_to_keep}\n",
    "    new_tokens = []\n",
    "    tokens = ' '.join(tokens)  # Convert list of tokens to a single string to make replacement easier\n",
    "    for n_gram, n_gram_with_underscore in n_grams_dict.items():\n",
    "        tokens = tokens.replace(n_gram, n_gram_with_underscore)\n",
    "    new_tokens = tokens.split()  # Split the string back into a list\n",
    "    return new_tokens\n",
    "\n",
    "data['combined_text'] = data['combined_text'].apply(lambda x: preserve_n_grams(x, n_grams_to_keep))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare the list of tokenized texts for Word2Vec\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "# Prepare the sentences for Word2Vec\n",
    "sentences = data['combined_text'].tolist()\n",
    "\n",
    "# Train the Word2Vec model\n",
    "model = Word2Vec(sentences, vector_size=100, window=5, min_count=1, workers=4, epochs=10)\n",
    "\n",
    "# Save the model for later use\n",
    "model.save(\"word2vec_model_news.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "queries = [\n",
    "    \"neural_network\",\n",
    "    \"deep_learning\",\n",
    "    \"reinforcement_learning\",\n",
    "    \"supervised_learning\",\n",
    "    \"unsupervised_learning\",\n",
    "    \"generative_model\",\n",
    "    \"natural_language_processing\",\n",
    "    \"artificial_intelligence\"\n",
    "]\n",
    "\n",
    "# Find and print nearest neighbors for each query\n",
    "for query in queries:\n",
    "    if query in model.wv.key_to_index:\n",
    "        print(f\"Nearest neighbors for '{query}':\")\n",
    "        for word, similarity in model.wv.most_similar(query, topn=10):\n",
    "            print(f\"{word}: {similarity:.4f}\")\n",
    "        print()\n",
    "    else:\n",
    "        print(f\"'{query}' not found in vocabulary.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open a file to write the nearest neighbors\n",
    "with open(\"nearest_neighbors_news.txt\", \"w\") as file:\n",
    "    for query in queries:\n",
    "        if query in model.wv.key_to_index:\n",
    "            file.write(f\"Nearest neighbors for '{query}':\\n\")\n",
    "            for word, similarity in model.wv.most_similar(query, topn=10):\n",
    "                file.write(f\"{word}: {similarity:.4f}\\n\")\n",
    "            file.write(\"\\n\")\n",
    "        else:\n",
    "            file.write(f\"'{query}' not found in vocabulary.\\n\")\n",
    "\n",
    "print(\"Nearest neighbors saved to 'nearest_neighbors_news.txt'.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from itertools import cycle\n",
    "\n",
    "# Define marker cycle\n",
    "markers = cycle(['*', 'o', 's', 'D', '^','X', '<','>'])  # Extend with more markers if needed\n",
    "\n",
    "# Map each query to a unique marker and retrieve top 5 nearest neighbors\n",
    "query_to_marker = {}\n",
    "all_words = set()\n",
    "query_to_nn = {}\n",
    "\n",
    "for query in queries:\n",
    "    query_to_marker[query] = next(markers)\n",
    "    nn = [word for word, _ in model.wv.most_similar(query, topn=7)]\n",
    "    query_to_nn[query] = nn\n",
    "    all_words.update([query] + nn)\n",
    "\n",
    "# Prepare word vectors and mapping for t-SNE\n",
    "words = list(all_words)\n",
    "word_vectors = np.array([model.wv[word] for word in words])\n",
    "word_to_index = {word: i for i, word in enumerate(words)}\n",
    "\n",
    "# t-SNE dimensionality reduction\n",
    "tsne = TSNE(n_components=2, random_state=42, perplexity=30, learning_rate=200, n_iter=3000)\n",
    "word_vectors_2d = tsne.fit_transform(word_vectors)\n",
    "\n",
    "# Plotting\n",
    "plt.figure(figsize=(20, 12))\n",
    "\n",
    "for query in queries:\n",
    "    query_index = word_to_index[query]\n",
    "    query_x, query_y = word_vectors_2d[query_index]\n",
    "    plt.scatter(query_x, query_y, color='red', marker=query_to_marker[query], alpha=0.6, edgecolor='k')\n",
    "    # Increase the font size for the query annotations\n",
    "    plt.annotate(query, (query_x, query_y), textcoords=\"offset points\", xytext=(0,10), ha='center', fontsize=12, weight='bold')\n",
    "\n",
    "    for nn in query_to_nn[query]:\n",
    "        nn_index = word_to_index[nn]\n",
    "        nn_x, nn_y = word_vectors_2d[nn_index]\n",
    "        # NN style checks\n",
    "        if nn in queries:  # NN is also a query\n",
    "            marker_style = query_to_marker[nn]\n",
    "        else:\n",
    "            marker_style = query_to_marker[query]  # Use the same marker as the query for visual grouping\n",
    "\n",
    "        plt.scatter(nn_x, nn_y, color='black', marker=marker_style, alpha=0.6)\n",
    "        plt.plot([query_x, nn_x], [query_y, nn_y], color='gray', linestyle='--', linewidth=0.5, alpha=0.5)\n",
    "        \n",
    "        # Increase the font size for the NN annotations\n",
    "        plt.annotate(nn, (nn_x, nn_y), textcoords=\"offset points\", xytext=(5,-5), ha='center', fontsize=10, alpha=0.9)\n",
    "\n",
    "plt.title(\"Word2Vec Embeddings Visualized with t-SNE\", fontsize=14)\n",
    "# Show the axes\n",
    "plt.axhline(y=0, color='k', linestyle='--', linewidth=0.5)\n",
    "plt.axvline(x=0, color='k', linestyle='--', linewidth=0.5)\n",
    "plt.xlabel('t-SNE dimension 1')\n",
    "plt.ylabel('t-SNE dimension 2')\n",
    "\n",
    "plt.savefig('nn_tsne_media.svg')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "End of script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "datasetId": 4414169,
     "sourceId": 7582888,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30646,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
